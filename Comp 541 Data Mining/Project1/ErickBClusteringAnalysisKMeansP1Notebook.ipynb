{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erick Barron\n",
    "#Introduction to the Dataset\n",
    "#added cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#added cell 2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#added cell 3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#added cell 4\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "#added cell 5\n",
    "from sklearn.metrics import silhouette_score\n",
    "#added in cell 6\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Load the dataset\n",
    "colleges = pd.read_csv('College.csv', index_col=0)\n",
    "\n",
    "#display first few rows using head()\n",
    "print(colleges.head())\n",
    "\n",
    "#Display a summary of dataset\n",
    "print(colleges.info())\n",
    "colleges.describe()\n",
    "\n",
    "#initial observations are that it does what it says it will\n",
    "#this step was pretty straightforward\n",
    "#It takes the data and prints the first 5 colleges\n",
    "#It also breaks up the info into a more readable form that displays relevent info together\n",
    "#however maybe that is specific to this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#check if there are any missing values can see which method you should use\n",
    "missing_values = colleges.isnull().sum()\n",
    "print(\"num of missing values:\\n\", missing_values)\n",
    "\n",
    "#using imputation which I chose since there was no missing values\n",
    "#I wanted to keep as much data as possible so decided on this method\n",
    "#This also encoded the data I changed the yes/no for private since it was causing me trouble and not running\n",
    "colleges_encoded = pd.get_dummies(colleges, columns=['Private'])\n",
    "\n",
    "#Normalize the data using minmax since I read that if we use columns it is better due to bounding it\n",
    "scaler = MinMaxScaler()\n",
    "colleges_normalized = pd.DataFrame(scaler.fit_transform(colleges_encoded), columns=colleges_encoded.columns)\n",
    "\n",
    "#Here we show the statistics after preprocessing\n",
    "print(\"Before Imputation and Encoding:\\n\", colleges.describe())\n",
    "print(\"\\nAfter Imputation, Encoding, and Normalization:\\n\", colleges_normalized.describe())\n",
    "#The output shows that the data has no missing values\n",
    "#also we can see the data as it was originally and the data after our processes \n",
    "#it is more readable and easier to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis\n",
    "#Histogram\n",
    "#We get the random data to analyze\n",
    "data1 = np.random.randn(1000)\n",
    "data2 = np.random.normal(loc=3, scale=1, size=1000)\n",
    "#Create the histogram \n",
    "plt.hist([data1, data2], bins=30, stacked=True, color=['Red', 'Blue'], edgecolor='black')\n",
    "#Add labels to histogram and display\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "plt.legend(['Datatset 1', 'Dataset 2'])\n",
    "plt.show()\n",
    "\n",
    "#Boxplots\n",
    "#Create dataset essentially find random numbers to use\n",
    "np.random.seed(10)\n",
    "data_1 = np.random.normal(100, 10, 200)\n",
    "data_2 = np.random.normal(90, 20, 200)\n",
    "data_3 = np.random.normal(80, 30, 200)\n",
    "data_4 = np.random.normal(70, 40, 200)\n",
    "data = [data_1, data_2, data_3, data_4]\n",
    " \n",
    "fig = plt.figure(figsize =(10, 6))\n",
    " \n",
    "#Create axes instance and plot then print\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "bp = ax.boxplot(data)\n",
    "plt.show()\n",
    "\n",
    "#ScatterPlots\n",
    "#Create random data\n",
    "x = np.random.rand(50)\n",
    "y = np.random.rand(50)\n",
    "colors = np.random.rand(50)\n",
    "sizes = 100 * np.random.rand(50)\n",
    " \n",
    "#Create a scatter plot\n",
    "plt.scatter(x, y, c=colors, s=sizes, alpha=0.7, cmap='Greens')\n",
    " \n",
    "#Add labels to scatter plot\n",
    "plt.title(\"Scatter Plot\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.colorbar(label='Color Intensity')\n",
    "#Print\n",
    "plt.show()\n",
    "\n",
    "#Summary Statistics compute and display important statistics\n",
    "summary_stats = colleges.describe()\n",
    "print(\"\\nSummary Statistics:\\n\", summary_stats)\n",
    "\n",
    "#Correlation matrix and heatmap\n",
    "correlation_matrix = colleges_normalized.corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "#This section was challenging but straightforward just create the graphs its cool to play around with\n",
    "#The statistics seems to be the same as the first section so maybe I did something wrong but not sure\n",
    "#The heatmap I should study to see exaclty what it is saying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of K-Means clustering\n",
    "#Choose the number of clusters using the elbow method\n",
    "inertia = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=20)\n",
    "    kmeans.fit(colleges_normalized)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "#Plot using the the elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 10), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "#Our optimal number of clusters\n",
    "optimal_clusters = 3 \n",
    "\n",
    "#Apply K-means clustering with the optimal clusters\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=20)\n",
    "kmeans.fit(colleges_normalized)\n",
    "\n",
    "#Display cluster centers and labels\n",
    "cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=colleges_encoded.columns)\n",
    "print(\"\\nCluster Centers:\\n\", cluster_centers)\n",
    "\n",
    "#Add cluster labels\n",
    "#Display the final clustering results\n",
    "colleges['Cluster_Labels'] = kmeans.labels_\n",
    "print(\"\\nFinal Results:\\n\", colleges[['Cluster_Labels']])\n",
    "\n",
    "#Visualization of clustered data\n",
    "#x and y are columns you want to analyze in the dataset \n",
    "#for this example I am analyzing acceptance and enrollement\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Accept', y='Enroll', hue='Cluster_Labels', data=colleges, palette='viridis', legend='full')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.show()\n",
    "\n",
    "#The elbow shows a reduction in inertia signaling lower cluster qualities\n",
    "#3 clusters is optimal as it seems to be a good sweet spot before lower quality and oversimplifying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of Clusters\n",
    "#Silhouette Score\n",
    "#measure how similar an object is to won cluster as opposed to others\n",
    "silhouette_avg = silhouette_score(colleges_normalized, kmeans.labels_)\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg}\")\n",
    "\n",
    "#Within-cluster sum of squares aka Inertia\n",
    "#check how coherent clusters are. Balance low Inertia and good number of clusters\n",
    "#Lower Inertia is better defined, but higher K means lower Inertia\n",
    "inertia_value = kmeans.inertia_\n",
    "print(f\"\\nWithin-cluster Sum of Squares or Inertia: {inertia_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "#Apply PCA to reduce dimension to 2D\n",
    "pca = PCA(n_components=2)\n",
    "colleges_pca = pd.DataFrame(pca.fit_transform(colleges_normalized), columns=['PC1', 'PC2'])\n",
    "\n",
    "#Concatenate cluster labels to the reduced-dimensional dataset\n",
    "colleges_pca['Cluster_Labels'] = kmeans.labels_\n",
    "\n",
    "#Plot clusters in reduced-dimensional\n",
    "#good for seeing patterns taht would be hard to see otherwise\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_Labels', data=colleges_pca, palette='viridis', legend='full')\n",
    "plt.title('K-means Clustering Results in Reduced-dimensional Space (PCA)')\n",
    "plt.show()\n",
    "\n",
    "#We use PCA since SVD is for sparse data\n",
    "#Each point is a college\n",
    "#The more seperated the clusters the better the algorithm is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation and Insights\n",
    "One thing I would like to point out is the heatmap shows correlation closer 1 being good and -1 bad with 0 being neutral.\n",
    "Darker colors are stronger correlated and lighter weaker.\n",
    "This insight may help us understand colleges better in the way acceptance is used, graduation rates, needs, etc.\n",
    "Using this data is important for understanding colleges and seeing trends we normally wouldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection\n",
    "I thought this was a fun assignment as I have never used any of the software used.\n",
    "Getting the results gave me a sense of accomplishment and wonder. The capabilities of programs is amazing.\n",
    "I found learning and using python was not difficult although I did have to research and learn on the go.\n",
    "I think playing around with different datasets will improve my understanding more.\n",
    "Upon completeing the assignment I think I will study clustering and how exactly it should be read and interpreted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
