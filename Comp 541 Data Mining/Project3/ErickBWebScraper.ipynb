{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erick Barron\n",
    "#Comp 541\n",
    "#Prof. Klotzman\n",
    "\n",
    "#Enviornment Setup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEB SCRAPING\n",
    "\n",
    "#FETCHING WEBPAGE CONTENT\n",
    "\n",
    "#get url and total pages on website\n",
    "mainurl = 'http://quotes.toscrape.com'\n",
    "allPages = 10\n",
    "    \n",
    "#PARSING HTML CONTENT\n",
    "\n",
    "#initialize empty list to store scrapped data with relevant items\n",
    "quotes = []\n",
    "authors = []\n",
    "tags = []\n",
    "\n",
    "#scrape data for all pages atarting from 1 to 10\n",
    "for page in range(1, allPages + 1):\n",
    "    \n",
    "    #create url for page and send request to url\n",
    "    url = mainurl.format(page)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #if response 200 good (seems to be the code for sucess in documentation)\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        #extract html and create beautiful soup to parse extracted data\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        #create loop for each quote and extract quote, author, and tags(need to inspect webpage to find if div, small, etc.)\n",
    "        #if not found create unknown\n",
    "        for quote in soup.find_all('div', class_='quote'):\n",
    "            webText = quote.find('span', class_='text')\n",
    "            if webText:\n",
    "                text = webText.get_text(strip=True)\n",
    "            else:\n",
    "                text = 'Unknown Quote'\n",
    "\n",
    "            webAuthor = quote.find('small', class_='author')\n",
    "            if webAuthor:\n",
    "                author = webAuthor.get_text(strip=True)\n",
    "            else:\n",
    "                author = 'Unknown Author'\n",
    "\n",
    "            tagList = quote.find('div', class_='tags').find_all('a')\n",
    "            tagText = ', '.join(tag.get_text(strip=True) for tag in tagList)\n",
    "            \n",
    "            #add the extracted data to their list\n",
    "            quotes.append(text)\n",
    "            authors.append(author)\n",
    "            tags.append(tagText)\n",
    "    #if not the correct code reject and print page it failed and code\n",
    "    else:\n",
    "        print(f\"could not get page {page}. Status code: {response.status_code}\")\n",
    "\n",
    "#organize scraped into one \n",
    "scrapedData = {'Quotes': quotes, 'Authors': authors, 'Tags': tags}\n",
    "\n",
    "#SAVING SCRAPED DATA\n",
    "\n",
    "#use pandas to organize data and create csv file\n",
    "df = pd.DataFrame(scrapedData)\n",
    "df.to_csv('ErickBScrapedInfo.csv', index=False)\n",
    "print(\"Data saved to CSV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA VISUALIZATION\n",
    "#DATA PREP\n",
    "\n",
    "#join all scraped quotes into a single string\n",
    "text = ' '.join(quotes)\n",
    "\n",
    "#CREATE WORDCLOUD\n",
    "\n",
    "#create wordcloud can change size color and max words that can appear\n",
    "wordcloud = WordCloud(\n",
    "    width=800, \n",
    "    height=400, \n",
    "    background_color='navy', \n",
    "    colormap='magma',\n",
    "    max_words=100,\n",
    ").generate(text)\n",
    "\n",
    "#VISUALIZATION\n",
    "\n",
    "#create the wordcloud output\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Scraped Quotes!', fontsize=20, color='black')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will explain the roles of the libraries. requests was used to send HTTPS requests which allowed us to get the html data. BeautifulSoup is for parsing the data allowing us to navigate and extract the data we recieve. Pandas is what we used for the dataframe to create the csv and is very popular we have used it in every project. MatPlotLib is for create visualization in this case our wordcloud, but we have also used it in other ways. WordCloud is for creating a wordcloud from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
